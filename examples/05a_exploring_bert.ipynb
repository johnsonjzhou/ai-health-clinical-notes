{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "358956fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    "    BertForMaskedLM,\n",
    "    BertForSequenceClassification,\n",
    "    # BertForMultipleChoice,\n",
    "    # BertForTokenClassification,\n",
    "    # BertForQuestionAnswering,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bde9d0",
   "metadata": {},
   "source": [
    "### Base Model\n",
    "\n",
    "For complete guide, refer to the [documentation](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForQuestionAnswering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2455964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "base_model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "print(base_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee09ff",
   "metadata": {},
   "source": [
    "### Masked language modeling\n",
    "\n",
    "Example: `[CLS] the quick brown fox [MASK] over the wall [SEP]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd45e4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (transform_act_fn): GELUActivation()\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mlm_model = BertForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "print(mlm_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a65f0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoded as tokens:\n",
      "tensor([[  101, 13107,  2003,  1996,   103,  1997,  2660,   102,     0,     0],\n",
      "        [  101,  1996,  4248,  2829,  4419,   103,  2058,  1996,  2813,   102],\n",
      "        [  101,  6207,   103,  2003, 12090,   102,     0,     0,     0,     0],\n",
      "        [  101,  4940,  2003,  1996,   103,  2103,  1997,  3848,   102,     0]])\n",
      "Special characters used by the model:\n",
      "[CLS]\n",
      "[SEP]\n",
      "[MASK]\n"
     ]
    }
   ],
   "source": [
    "inputs = [\n",
    "    \"canberra is the [MASK] of australia\",\n",
    "    \"the quick brown fox [MASK] over the wall\",\n",
    "    \"apple [MASK] is delicious\",\n",
    "    ]\n",
    "encoded_inputs = tokenizer(inputs, padding=True, truncation=True, max_length=50, return_tensors=\"pt\")\n",
    "print(\"Text encoded as tokens:\")\n",
    "print(encoded_inputs.input_ids)\n",
    "print(\"Special characters used by the model:\")\n",
    "print(tokenizer.decode(101))\n",
    "print(tokenizer.decode(102))\n",
    "print(tokenizer.decode(103))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c3295e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for the MASK:\n",
      "capital jumped pie capital\n"
     ]
    }
   ],
   "source": [
    "mask_positions = encoded_inputs.input_ids == 103\n",
    "output = mlm_model(**encoded_inputs)\n",
    "masked_output = output.logits[mask_positions]\n",
    "masked_texts = tokenizer.decode(masked_output.argmax(dim=-1))\n",
    "print(\"Predictions for the MASK:\")\n",
    "print(masked_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d375b00",
   "metadata": {},
   "source": [
    "### Sequence classification\n",
    "\n",
    "Example: fine-tune to predict the class label of input text, such as `[CLS] hello, my dog is cute [SEP]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccb76ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cls_model = BertForSequenceClassification.from_pretrained(\n",
    "    \"google-bert/bert-base-uncased\",\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    num_labels=3\n",
    "    )\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "print(cls_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8255c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3836, 0.3341, 0.2823], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([0.4005, 0.3623, 0.2373], grad_fn=<SoftmaxBackward0>) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "inputs = [\n",
    "    \"hello, my dog is cute\",\n",
    "    \"the weather is nice\"\n",
    "]\n",
    "encoded_inputs = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output         = cls_model(**encoded_inputs)\n",
    "for i in range(output.logits.shape[0]):\n",
    "    logits = output.logits[i]\n",
    "    pred_proba = nn.functional.softmax(logits, dim=0)\n",
    "    pred_class = torch.argmax(pred_proba)\n",
    "    print(pred_proba, pred_class)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
